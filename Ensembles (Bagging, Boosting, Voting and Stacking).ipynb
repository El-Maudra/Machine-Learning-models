{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Daily Time Spent on Site</th>\n",
       "      <th>Age</th>\n",
       "      <th>Area Income</th>\n",
       "      <th>Daily Internet Usage</th>\n",
       "      <th>Ad Topic Line</th>\n",
       "      <th>City</th>\n",
       "      <th>Male</th>\n",
       "      <th>Country</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Clicked on Ad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>80.23</td>\n",
       "      <td>31</td>\n",
       "      <td>68094.85</td>\n",
       "      <td>196.23</td>\n",
       "      <td>Secured clear-thinking middleware</td>\n",
       "      <td>South Daniellefort</td>\n",
       "      <td>0</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>2016-03-19 14:23:45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>57.51</td>\n",
       "      <td>38</td>\n",
       "      <td>47682.28</td>\n",
       "      <td>105.71</td>\n",
       "      <td>Re-engineered zero-defect open architecture</td>\n",
       "      <td>Jeffreymouth</td>\n",
       "      <td>0</td>\n",
       "      <td>Moldova</td>\n",
       "      <td>2016-03-31 08:53:43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>82.38</td>\n",
       "      <td>35</td>\n",
       "      <td>25603.93</td>\n",
       "      <td>159.60</td>\n",
       "      <td>Polarized analyzing intranet</td>\n",
       "      <td>Port Blake</td>\n",
       "      <td>0</td>\n",
       "      <td>Spain</td>\n",
       "      <td>2016-07-18 01:36:37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Daily Time Spent on Site  Age  Area Income  Daily Internet Usage  \\\n",
       "172                     80.23   31     68094.85                196.23   \n",
       "820                     57.51   38     47682.28                105.71   \n",
       "466                     82.38   35     25603.93                159.60   \n",
       "\n",
       "                                   Ad Topic Line                City  Male  \\\n",
       "172            Secured clear-thinking middleware  South Daniellefort     0   \n",
       "820  Re-engineered zero-defect open architecture        Jeffreymouth     0   \n",
       "466                 Polarized analyzing intranet          Port Blake     0   \n",
       "\n",
       "     Country            Timestamp  Clicked on Ad  \n",
       "172    Qatar  2016-03-19 14:23:45              0  \n",
       "820  Moldova  2016-03-31 08:53:43              1  \n",
       "466    Spain  2016-07-18 01:36:37              1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ads = pd.read_csv('Advertising.csv')\n",
    "ads.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads.drop(['Ad Topic Line', 'City', 'Country','Timestamp'], inplace= True, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ads[['Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage', 'Male']]\n",
    "y = ads['Clicked on Ad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = 2\\alpha^2 + \\beta + \\phi - \\int\\sec$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[97  1]\n",
      " [ 8 94]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.96        98\n",
      "           1       0.99      0.92      0.95       102\n",
      "\n",
      "    accuracy                           0.95       200\n",
      "   macro avg       0.96      0.96      0.95       200\n",
      "weighted avg       0.96      0.95      0.95       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_model = RandomForestClassifier(n_estimators = 10)\n",
    "rfc_model.fit(X_train, y_train)\n",
    "rfc_preds = rfc_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, rfc_preds))\n",
    "print(classification_report(y_test, rfc_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[93  5]\n",
      " [ 6 96]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94        98\n",
      "           1       0.95      0.94      0.95       102\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.94      0.95      0.94       200\n",
      "weighted avg       0.95      0.94      0.95       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_model = RandomForestClassifier(n_estimators = 3)\n",
    "rfc_model.fit(X_train, y_train)\n",
    "rfc_preds = rfc_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, rfc_preds))\n",
    "print(classification_report(y_test, rfc_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[95  3]\n",
      " [ 5 97]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96        98\n",
      "           1       0.97      0.95      0.96       102\n",
      "\n",
      "    accuracy                           0.96       200\n",
      "   macro avg       0.96      0.96      0.96       200\n",
      "weighted avg       0.96      0.96      0.96       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_model = RandomForestClassifier(n_estimators = 5)\n",
    "rfc_model.fit(X_train, y_train)\n",
    "rfc_preds = rfc_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, rfc_preds))\n",
    "print(classification_report(y_test, rfc_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[106   3]\n",
      " [  6  85]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       109\n",
      "           1       0.97      0.93      0.95        91\n",
      "\n",
      "    accuracy                           0.95       200\n",
      "   macro avg       0.96      0.95      0.95       200\n",
      "weighted avg       0.96      0.95      0.95       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_model = RandomForestClassifier(n_estimators = 20)\n",
    "rfc_model.fit(X_train, y_train)\n",
    "rfc_preds = rfc_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, rfc_preds))\n",
    "print(classification_report(y_test, rfc_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[104   5]\n",
      " [  5  86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       109\n",
      "           1       0.95      0.95      0.95        91\n",
      "\n",
      "    accuracy                           0.95       200\n",
      "   macro avg       0.95      0.95      0.95       200\n",
      "weighted avg       0.95      0.95      0.95       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ABC = AdaBoostClassifier(n_estimators = 10)\n",
    "ABC.fit(X_train, y_train)\n",
    "ABC_preds = ABC.predict(X_test)\n",
    "print(confusion_matrix(y_test, ABC_preds))\n",
    "print(classification_report(y_test, ABC_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:12:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[105   4]\n",
      " [  5  86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       109\n",
      "           1       0.96      0.95      0.95        91\n",
      "\n",
      "    accuracy                           0.95       200\n",
      "   macro avg       0.96      0.95      0.95       200\n",
      "weighted avg       0.96      0.95      0.95       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRFClassifier\n",
    "xgb_model = XGBRFClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_model_preds = xgb_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, xgb_model_preds))\n",
    "print(classification_report(y_test, xgb_model_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:39:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[107   2]\n",
      " [  5  86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       109\n",
      "           1       0.98      0.95      0.96        91\n",
      "\n",
      "    accuracy                           0.96       200\n",
      "   macro avg       0.97      0.96      0.96       200\n",
      "weighted avg       0.97      0.96      0.96       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "vc_model = VotingClassifier(estimators=[('xgb', xgb_model), ('ABC', ABC), ('rf', rfc_model)], voting='hard')\n",
    "vc_model.fit(X_train, y_train)\n",
    "vc_pred = vc_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, vc_pred))\n",
    "print(classification_report(y_test, vc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:40:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[106   3]\n",
      " [  5  86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       109\n",
      "           1       0.97      0.95      0.96        91\n",
      "\n",
      "    accuracy                           0.96       200\n",
      "   macro avg       0.96      0.96      0.96       200\n",
      "weighted avg       0.96      0.96      0.96       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "vc_model = VotingClassifier(estimators=[('xgb', xgb_model), ('ABC', ABC), ('rf', rfc_model)], voting='soft')\n",
    "vc_model.fit(X_train, y_train)\n",
    "vc_pred = vc_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, vc_pred))\n",
    "print(classification_report(y_test, vc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:35:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:35:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:35:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:35:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:35:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:35:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[106   3]\n",
      " [  5  86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       109\n",
      "           1       0.97      0.95      0.96        91\n",
      "\n",
      "    accuracy                           0.96       200\n",
      "   macro avg       0.96      0.96      0.96       200\n",
      "weighted avg       0.96      0.96      0.96       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "sc_model = StackingClassifier(estimators=[('xgb', xgb_model), ('ABC', ABC), ('rf', rfc_model)])\n",
    "sc_model.fit(X_train, y_train)\n",
    "sc_pred = sc_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, sc_pred))\n",
    "print(classification_report(y_test, sc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
